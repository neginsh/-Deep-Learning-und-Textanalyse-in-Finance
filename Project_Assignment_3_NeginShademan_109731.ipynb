{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZCwK57CXrJ9W",
        "_oF7JJGMddSJ",
        "iAXE0LYujZkr",
        "CyHZqGTsqUhS",
        "nrCQ2AZHrj8i"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neginsh/Deep-Learning-und-Textanalyse-in-Finance/blob/main/Project_Assignment_3_NeginShademan_109731.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please send an email to lukas.marx@uni-passau.de who will send you a csv-file including earning call transcripts for a single company. Using the pretrained Word2Vec and Doc2Vec model (https://github.com/RalfKellner/dlta_ec2vec) and code snippets from the course, conduct the following tasks.\n",
        "\n",
        "1. Extract questions and answers from each earning call transcript\n",
        "2. Convert each question and answer to a document vector\n",
        "3. For each document find the five closest documents (close means highest cosine similarity) and present  1-3 documents (that seem reasonable) and its top five similar documents. Describe if you think that the documents discuss similar content.\n",
        "4. Given the word list below, determine the cosine similarity between every word and document vector. Select 1-3 words with high similarity to documents and describe if you think that these documents relate to a topic which the word could represent.\n",
        "5. Find the five documents which are closest to the words 'expectations' or 'expect' by using vector embeddings.\n",
        "\n",
        "Please send your results as a ipynb-Notebook to lukas.marx@uni-passau.de no later than March 15, 2023 (2 PM/ 14 Uhr CET)."
      ],
      "metadata": {
        "id": "eIDX10EF7i5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnGjRL_t7avI"
      },
      "outputs": [],
      "source": [
        "# esg words taken from table 3 in Baier et al. (2020): Environmental, social and governance reporting in annual reports: A textual analysis\n",
        "\n",
        "word_list = ['clean', 'environmental', 'epa', 'sustainability', 'climate', 'warming', 'biofuels', 'biofuel',\n",
        "'green', 'renewable', 'solar', 'stewardship', 'wind', 'emission', 'emissions', 'ghg', 'ghgs', 'greenhouse', 'atmosphere', 'emit',\n",
        "'agriculture', 'deforestation', 'pesticide', 'pesticides', 'wetlands', 'zoning', 'biodiversity', 'species',\n",
        "'wilderness', 'wildlife', 'freshwater', 'groundwater', 'water', 'air', 'carbon', 'nitrogen', 'pollution',\n",
        "'superfund', 'hazardous', 'householding', 'pollutants', 'printing', 'recycling', 'toxic', 'waste', 'wastes', 'weee','recycle',\n",
        "'align', 'aligned', 'aligning', 'alignment', 'aligns', 'bylaw', 'bylaws', 'charter', 'charters', 'culture', 'death', 'duly', 'parents', 'independent', \n",
        "'compliance', 'conduct', 'conformity', 'governance', 'misconduct', 'parachute', 'parachutes', 'perquisites', 'plane', 'planes', 'poison', 'retirement',\n",
        "'approval', 'approvals', 'approve', 'approved',\n",
        "'approves', 'approving', 'assess', 'assessed', 'assesses',\n",
        "'assessing', 'assessment', 'assessments', 'audit', 'audited',\n",
        "'auditing', 'auditor', 'auditors', 'audits', 'control', 'controls', 'coso',\n",
        "'detect', 'detected', 'detecting', 'detection', 'evaluate',\n",
        "'evaluated', 'evaluates', 'evaluating', 'evaluation', 'evaluations',\n",
        "'examination', 'examinations', 'examine', 'examined',\n",
        "'examines', 'examining', 'irs', 'oversee', 'overseeing', 'oversees',\n",
        "'oversight', 'review', 'reviewed', 'reviewing', 'reviews', 'rotation',\n",
        "'test', 'tested', 'testing', 'tests', 'treadway',\n",
        "'backgrounds', 'independence', 'leadership',\n",
        "'nomination', 'nominations', 'nominee', 'nominees',\n",
        "'perspectives', 'qualifications', 'refreshment', 'skill', 'skills',\n",
        "'succession', 'tenure', 'vacancies', 'vacancy'\n",
        "'appreciation', 'award', 'awarded', 'awarding',\n",
        "'awards', 'bonus', 'bonuses', 'cd', 'compensate', 'compensated',\n",
        "'compensates', 'compensating', 'compensation', 'eip', 'iso', 'isos',\n",
        "'payout', 'payouts', 'pension', 'prsu', 'prsus', 'recoupment',\n",
        "'remuneration', 'reward', 'rewarding', 'rewards', 'rsu', 'rsus',\n",
        "'salaries', 'salary', 'severance', 'vest', 'vested', 'vesting', 'vests'\n",
        "'ballot', 'ballots', 'cast', 'consent', 'elect',\n",
        "'elected', 'electing', 'election', 'elections', 'elects', 'nominate',\n",
        "'nominated', 'plurality', 'proponent', 'proponents', 'proposal',\n",
        "'proposals', 'proxies', 'quorum', 'vote', 'voted', 'votes', 'voting',\n",
        "'brother', 'clicking', 'conflict', 'conflicts', 'family',\n",
        "'grandchildren', 'grandparent', 'grandparents', 'inform',\n",
        "'insider', 'insiders', 'inspector', 'inspectors', 'interlocks',\n",
        "'nephews', 'nieces', 'posting', 'relatives', 'siblings', 'sister', 'son',\n",
        "'spousal', 'spouse', 'spouses', 'stepchildren', 'stepparents',\n",
        "'transparency', 'transparent', 'visit', 'visiting', 'visits', 'webpage', 'website',\n",
        "'attract', 'attracting', 'attracts', 'incentive', 'incentives',\n",
        "'interview', 'interviews', 'motivate', 'motivated', 'motivates',\n",
        "'motivating', 'motivation', 'recruit', 'recruiting', 'recruitment',\n",
        "'retain', 'retainer', 'retainers', 'retaining', 'retention', 'talent', 'talented', 'talents',\n",
        "'bribery', 'corrupt', 'corruption', 'crimes', 'embezzlement',\n",
        "'grassroots', 'influence', 'influences', 'influencing', 'lobbied', 'lobbies', 'lobby', 'lobbying', 'lobbyist', 'lobbyists',\n",
        "'cobc', 'ethic', 'ethical', 'ethically', 'ethics', 'honesty', 'whistleblower',\n",
        "'announce', 'announced', 'announcement', 'announcements', 'announces', 'announcing', 'communicate', 'communicated',\n",
        "'communicates', 'communicating', 'erm', 'fairly', 'integrity', 'liaison', 'presentation', 'presentations', 'sustainable', \n",
        "'asc', 'disclose', 'disclosed', 'discloses', 'disclosing', 'disclosure', 'disclosures', 'fasb', 'gaap',\n",
        "'objectivity', 'press', 'sarbanes',  'engagement', 'engagements',\n",
        "'feedback', 'hotline', 'investor', 'invite', 'invited', 'mail', 'mailed',\n",
        "'mailing', 'mailings', 'notice', 'relations', 'stakeholder', 'stakeholders',\n",
        "'compact', 'ungc', 'citizen', 'citizens', 'csr', 'disabilities', 'disability', 'disabled',\n",
        "'human', 'nations', 'social', 'un', 'veteran', 'veterans', 'vulnerable',\n",
        "'children', 'epidemic', 'health', 'healthy', 'ill', 'illness',\n",
        "'pandemic', 'childbirth', 'drug', 'medicaid', 'medicare', 'medicine','medicines', 'hiv',\n",
        "'alcohol', 'drinking', 'bugs', 'conformance', 'defects', 'fda',\n",
        "'inspection', 'inspections', 'minerals', 'standardization', 'warranty',\n",
        "'dignity', 'discriminate', 'discriminated', 'discriminating', 'discrimination', 'equality', 'freedom',\n",
        "'humanity', 'nondiscrimination', 'sexual', 'communities', 'community',\n",
        "'expression', 'marriage', 'privacy', 'peace',\n",
        "'bargaining', 'eeo', 'fairness', 'fla', 'harassment',\n",
        "'injury', 'labor', 'overtime', 'ruggie', 'sick', 'wage', 'wages',\n",
        "'workplace', 'bisexual', 'diversity', 'ethnic', 'ethnically', 'ethnicities',\n",
        "'ethnicity', 'female', 'females', 'gay', 'gays', 'gender', 'genders',\n",
        "'homosexual', 'immigration', 'lesbian', 'lesbians', 'lgbt',\n",
        "'minorities', 'minority', 'ms', 'race', 'racial', 'religion', 'religious',\n",
        "'sex', 'transgender', 'woman', 'women',\n",
        "'occupational', 'safe', 'safely', 'safety', 'ilo', 'labour', 'eicc',\n",
        "'endowment', 'endowments', 'people', 'philanthropic',\n",
        "'philanthropy', 'socially', 'societal', 'society', 'welfare',\n",
        "'charitable', 'charities', 'charity', 'donate', 'donated',\n",
        "'donates', 'donating', 'donation', 'donations', 'donors',\n",
        "'foundation', 'foundations', 'gift', 'gifts', 'nonprofit', 'poverty',\n",
        "'courses', 'educate', 'educated', 'educates',\n",
        "'educating', 'education', 'educational', 'learning', 'mentoring',\n",
        "'scholarships', 'teach', 'teacher', 'teachers', 'teaching', 'training',\n",
        "'employ', 'employment', 'headcount', 'hire', 'hired',\n",
        "'hires', 'hiring', 'staffing', 'unemployment']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negin Shademan\n",
        "\n",
        "Matrikelnummer:\n",
        "109731\n",
        "\n",
        "\n",
        "---\n",
        "# Loading the dataset and importing the necessary libraries"
      ],
      "metadata": {
        "id": "ZCwK57CXrJ9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snbvq0S4ZD_B",
        "outputId": "021d3ab3-6077-4020-ddc9-8ac6bf9e6caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.22.4)\n",
            "Collecting FuzzyTM>=0.4.0\n",
            "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.10.1)\n",
            "Collecting pyfume\n",
            "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from FuzzyTM>=0.4.0->gensim) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
            "Collecting simpful\n",
            "  Downloading simpful-2.10.0-py3-none-any.whl (31 kB)\n",
            "Collecting fst-pso\n",
            "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->FuzzyTM>=0.4.0->gensim) (1.15.0)\n",
            "Collecting miniful\n",
            "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
            "Building wheels for collected packages: fst-pso, miniful\n",
            "  Building wheel for fst-pso (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20443 sha256=7124b3bfe76098c09634042bd3f8b83be2b99146e8f0a416d5d79b7412374093\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/65/c4/d27eeee9ba3fc150a0dae150519591103b9e0dbffde3ae77dc\n",
            "  Building wheel for miniful (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3530 sha256=bfab0e819477c60c26510608734e6dcff7821fd8e358975c45b1fb89776b1640\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/d9/a0/ddd93af16d5855dd9bad417623e70948fdac119d1d34fb17c8\n",
            "Successfully built fst-pso miniful\n",
            "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 simpful-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas==1.5.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJNCCWw0bObq",
        "outputId": "2c57720b-c4ad-4436-982e-1273f0c21e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab as pl\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import nltk\n",
        "from gensim.models import Doc2Vec\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "AphRKBkLrKn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCtyToFgrZgw",
        "outputId": "d00c87b0-bd4a-4cb7-fa67-8ea3bcfc82cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcripts = pd.read_csv('/content/drive/MyDrive/shadem01.csv')"
      ],
      "metadata": {
        "id": "zb7ZxIsArdjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/RalfKellner/dlta_ec2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZA-j13Bw1rh",
        "outputId": "e4a06642-b404-4b17-a053-74f55cfe26fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dlta_ec2vec'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (6/6), 50.72 MiB | 9.40 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the dataset\n",
        "\n",
        "The data we are working with is all from UnitedHealth Group Incorporated earning calls. In the dataframe we have feature such as date and the content of earning calls\n",
        "\n",
        "For the task of this assingment we will only work with content column of this dataframe. In each call we have one operator who is controling the call and one or more company represantative who is answering questions and multiple people who ask questions.\n",
        "\n",
        "In order to seperate questions from answers, we calculate how many time in each call each person has talked and we assume that the company represantative speaks more often than the analysts. In order to calculate this we use the code provided by the Prof. Kellner during the lectures. We will also use the already trained model and dictionary provided to us by Prof. Kellner.\n",
        "\n",
        "We also have a word list that we are working with so that the dimentions of vector is not too long."
      ],
      "metadata": {
        "id": "_oF7JJGMddSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "CBXYBXE7v2_c",
        "outputId": "a981420f-bae2-4c5a-dc70-79bbe7bad78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    index ticker  quarter  year                 date  \\\n",
              "0       0    UNH        2  2007  2007-07-19 17:00:00   \n",
              "1       1    UNH        3  2007  2007-10-18 17:00:00   \n",
              "2       2    UNH        4  2007  2008-01-22 17:00:00   \n",
              "3       3    UNH        1  2008  2008-04-22 17:00:00   \n",
              "4       4    UNH        2  2008  2008-07-22 17:00:00   \n",
              "..    ...    ...      ...   ...                  ...   \n",
              "57     57    UNH        3  2021  2021-10-14 13:21:06   \n",
              "58     58    UNH        4  2021  2022-01-19 11:59:06   \n",
              "59     59    UNH        1  2022  2022-04-14 10:18:03   \n",
              "60     60    UNH        2  2022  2022-07-15 11:53:10   \n",
              "61     61    UNH        3  2022  2022-10-14 12:44:03   \n",
              "\n",
              "                                              content  \n",
              "0   Operator: Good morning. My name is Dennis and ...  \n",
              "1   Operator: Good morning. My name is Dennis and ...  \n",
              "2   Operator: My name is Dennis, and I will beyour...  \n",
              "3   Operator: Good morning. My name is Dennis and ...  \n",
              "4   Operator: Good morning, my name is Dennis and ...  \n",
              "..                                                ...  \n",
              "57  Operator: Please standby, we're about to begin...  \n",
              "58  Operator: Good morning, and welcome to the Uni...  \n",
              "59  Operator: Good morning, and welcome to the Uni...  \n",
              "60  Operator: Good morning, and welcome to the Uni...  \n",
              "61  Operator: Good day, everyone and welcome to th...  \n",
              "\n",
              "[62 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-766cb99a-1ffb-443a-9091-0ad057f72bc4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>ticker</th>\n",
              "      <th>quarter</th>\n",
              "      <th>year</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>UNH</td>\n",
              "      <td>2</td>\n",
              "      <td>2007</td>\n",
              "      <td>2007-07-19 17:00:00</td>\n",
              "      <td>Operator: Good morning. My name is Dennis and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>UNH</td>\n",
              "      <td>3</td>\n",
              "      <td>2007</td>\n",
              "      <td>2007-10-18 17:00:00</td>\n",
              "      <td>Operator: Good morning. My name is Dennis and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>UNH</td>\n",
              "      <td>4</td>\n",
              "      <td>2007</td>\n",
              "      <td>2008-01-22 17:00:00</td>\n",
              "      <td>Operator: My name is Dennis, and I will beyour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>UNH</td>\n",
              "      <td>1</td>\n",
              "      <td>2008</td>\n",
              "      <td>2008-04-22 17:00:00</td>\n",
              "      <td>Operator: Good morning. My name is Dennis and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>UNH</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>2008-07-22 17:00:00</td>\n",
              "      <td>Operator: Good morning, my name is Dennis and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>57</td>\n",
              "      <td>UNH</td>\n",
              "      <td>3</td>\n",
              "      <td>2021</td>\n",
              "      <td>2021-10-14 13:21:06</td>\n",
              "      <td>Operator: Please standby, we're about to begin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>58</td>\n",
              "      <td>UNH</td>\n",
              "      <td>4</td>\n",
              "      <td>2021</td>\n",
              "      <td>2022-01-19 11:59:06</td>\n",
              "      <td>Operator: Good morning, and welcome to the Uni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>UNH</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "      <td>2022-04-14 10:18:03</td>\n",
              "      <td>Operator: Good morning, and welcome to the Uni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>60</td>\n",
              "      <td>UNH</td>\n",
              "      <td>2</td>\n",
              "      <td>2022</td>\n",
              "      <td>2022-07-15 11:53:10</td>\n",
              "      <td>Operator: Good morning, and welcome to the Uni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>61</td>\n",
              "      <td>UNH</td>\n",
              "      <td>3</td>\n",
              "      <td>2022</td>\n",
              "      <td>2022-10-14 12:44:03</td>\n",
              "      <td>Operator: Good day, everyone and welcome to th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-766cb99a-1ffb-443a-9091-0ad057f72bc4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-766cb99a-1ffb-443a-9091-0ad057f72bc4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-766cb99a-1ffb-443a-9091-0ad057f72bc4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ec_transcript = transcripts.content.values[-1]\n",
        "for par in ec_transcript.split('\\n'):\n",
        "  print(par)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-4BiI4R6UF7",
        "outputId": "9b218d89-08e9-43ac-daf8-71e5d225db35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operator: Good day, everyone and welcome to the UnitedHealth Group Third Quarter 2022 Earnings Conference Call. A question-and-answer session will follow UnitedHealth Groupâs prepared remarks. As a reminder, this call is being recorded. Here are some important introductory information. This call contains forward-looking statements under U.S. federal securities laws. These statements are subject to risks and uncertainties that could cause actual results to differ materially from historical experience or present expectations. A description of some of these risks and uncertainties can be followed in the reports that we file with the Securities and Exchange Commission, including the cautionary statements included in our current and periodic filings. This call will also reference non-GAAP amounts. A reconciliation of the non-GAAP to GAAP amounts is available on the financial and earnings reports section of the companyâs Investor Relations page at www.unitedhealthgroup.com. Information presented on this call is contained in the earnings release we issued this morning and in our Form 8-K dated October 14, 2022, which maybe accessed from the Investor Relations page of the companyâs website. I will now turn the call over to our Chief Executive Officer of UnitedHealth Group, Andrew Witty. Please go ahead.\r\n",
            "Andrew Witty: Thank you. Good morning and thank you all for joining us today. As we approach the final stretch of 2022, let me start by recognizing our colleagues, the people of Optum and UnitedHealthcare who continue to work diligently day in and day out for those we serve. Their efforts allow us to deliver durable and balanced growth and to increase our 2022 adjusted earnings outlook to a range of $21.85 to $22.05 per share. As an enterprise, we remain focused on our mission and on advancing our growth strategies. Our aim is to serve more people through value-based care and expanded health benefits offerings, a robust foundation on which to consistently drive strong growth into 2023 and beyond. Consumers want value, especially in the current economic environment and that means high quality care that is more accessible, more affordable and more responsive to their individual needs. In the past quarter, we have accelerated efforts to deliver on this critical consumer proposition, launching several initiatives to reach more people in more communities while deepening relationships with those we already serve. For example, in September, we announced a long-term collaboration with Walmart to provide Optum technology and expertise that will enable Americaâs largest retailer to provide value-based care to consumers in its clinics. Starting in 2023, we will jointly develop 15 Walmart health clinics in Florida and Georgia and will extend into additional geographies over time. As we expand the collaboration, there will be broad opportunities to address social determinants of health by improving access to benefits such as healthy foods, medications, dental and vision services and more. We also launched a distinctive partnership with Red Ventures, a digital media company connecting tens of millions of consumers each month to its clientsâ products and services, through a broad portfolio of proprietary digital content platforms. This partnership called RVO Health combines Red Ventures popular health and wellness platforms, including health line and health grades, with Optumâs consumer marketplace, the Optum Store and the Optum Perks prescription discount card. RVO Health will enable us to engage with more than 100 million active monthly visitors seeking the health advice and insights they need introducing them to relevant products and services through a customized end-to-end digital platform. Affordability is an essential component of value in healthcare, especially when it determines access to the life-saving medicines people need for themselves and their families. In July, we announced zero consumer cost share on drugs for diabetes, severe allergic reactions and other emergency situations starting in January for those we serve with commercial insured benefits. Now, we are working with self-funded employers who are exploring how they can provide these vital medications for zero co-pay. So far, this benefit will be available to more than 1 million additional people. And we are actively engaging with many additional employers. We are also improving access to essential medicines for those even without health benefits coverage. For example, Optum Store launched a new partnership with Sanofi to help consumers without insurance obtain insulin for $35 a month and they will be able to have it delivered to their home. Turning to health benefits, today, nearly half of American seniors are enrolled in Medicare Advantage plans compared to about 25% a decade ago. And MA plays a vital role in serving those consumers who are significantly more diverse, have lower incomes and more complex care needs than the average senior. There are compelling reasons why seniors increasingly choose MA. Through Medicare Advantage, people are experiencing better health outcomes than in traditional fee-for-service Medicare across a wide spectrum of measures. For example, MA members with diabetes have over 50% lower rates of any category of complication and over 70% lower rates of serious complication. This is due to our ability to provide deeper, more coordinated levels of care. And this is accomplished at lower cost. People served by MA spend as much as 40% less compared to those in Medicare fee-for-service. And this high value for people is delivered at a lower net cost to the government. We are confident our differentiated offerings will once again this year resonate with consumers who are even more focused on affordability, value and simplicity given the rising cost of daily life. Today, in the United States, more people than ever have access to health benefits, an important milestone on the path towards universal coverage, an objective we have long supported. Much of this expanded coverage has occurred in Medicaid. Looking to 2023 and given the potential resumption of eligibility redeterminations next year, a high priority for our team is assuring continuity of access and care for those we serve. The initiatives our team is pursuing to help assure continuous coverage include launching direct outreach and partnering with states and community organizations to identify those at risk and help them retain their benefit coverage. Partnering with national retailers and pharmacies to educate consumers about available coverage options and assistance while they are shopping in store and engaging with employers to extend annual enrollment periods and drive education efforts to employees who are eligible for coverage. Underpinning our growing consumer agenda is an ambitious multiyear effort to deepen and expand our enterprise technology capabilities. The recent combination of OptumInsight and Change Healthcare reflects our accelerating efforts to help create more effective and simple experiences for consumers, payers and care providers while lowering costs across the health system. I want to formally welcome our newest colleagues, the talented and compassionate team at Change Healthcare, with whom we have just started working to build upon our shared vision for a more effective and adaptive health system for all participants. With that, I will turn it over to President and Chief Operating Officer, Dirk McMahon.\r\n",
            "Dirk McMahon: Thank you, Andrew. As Andrew just mentioned, we are very excited about the recent combination of Change Healthcare and OptumInsight. With a grand total of 11 days of integration work behind us, I thought I would provide a bit more commentary on how together we can make the health system simpler and more efficient. Overall, Change brings a robust transaction network built on strong payer and provider connectivity. Together, our focus areas include: first, improving the quality of health care delivery by offering critical point-of-care insights aligned to evidence-based medical standards within the workflow of physicians; second, simplifying administration by fully automating claims transactions, including editing in the EDI stream, improving claim accuracy; lastly, we are reducing friction in the payment process by providing patient benefits and payment obligations at point of service and applying payment integrity at its upfront. Our teams have started out of the gate working intensively together to make these visions a reality and to create the next era of growth for OptumInsight. We are very confident that our combined capabilities will enable us to better serve all healthcare constituents. Beyond OptumInsight, as you might imagine, I spend a lot of time engaging customers of all sizes, operating in almost every sector and representing diverse employee populations. Across these customers, no matter the person or the industry, I hear the same consistent message. The need for deeper levels of support and more resources to comprehensively meet health needs in both the traditional, but also increasingly focused on behavioral health. Companies want to provide a broader range of resources and deeper levels of support for their employees and their families and the groundwork we have been building is resonating in this rapidly evolving area. Here is what we are doing about it. First, we are tackling access. We have expanded our behavioral health network by 25% over the last couple of years, including a growing complement of behavioral clinical practices owned and operated by Optum. And as many of you know, behavioral health is a 24/7 challenge. So we are continuing to expand our portfolio of digital offerings supporting a range of needs, allowing patients to get resources when they need them. We have also made significant improvements to help consumers access vital information more easily through improved navigation tools, guiding consumers to the appropriate condition-specific level of behavioral health is a challenge we are prioritizing. These new experiences have improved customer satisfaction and we are getting people to the right care more quickly. Behavioral health plays an integral role of the overall health and well-being of the people we serve. So you will all continue to â so you will continue to see us increasing access, quality and affordability in this clinically important and sensitive area. Another tenet of our consumer focus is meeting people where they are, which includes expanding our clinical capabilities to care for people, more holistically, in their homes. We know that at home care settings, especially for people with mobility challenges and highly complex health needs can improve outcomes, elevate patient experience and result in better care. So we bring together teams with medical, behavioral and palliative experience in addition to our home infusion capabilities of OptumRx. By doing so, we help patients and their families keep multiple chronic conditions in check while significantly reducing the need for care in acute and post-acute settings, a real positive for them. The expanding clinical breadth and deepening integration of our value-based care offerings are moving us beyond the confiance of traditional clinical settings, creating an opportunity for us to serve more people more effectively. With that, now I will turn it over to Chief Financial Officer, John Rex.\r\n",
            "John Rex: Thank you, Dirk. As those of you listening know well, numbers can tell a story and the story our numbers tell is one of broad-based growth and substantial near and long-term potential. So, let me walk through some of those numbers with you. In the third quarter, UnitedHealth Group revenues of $81 billion grew 12% or $8.6 billion, highlighted by broad-based double-digit growth at both Optum and UnitedHealthcare. Care patterns in the quarter remained similar to those of the second quarter and our planning for next year anticipates care patterns continue to normalize. We are encouraged to see people obtaining preventive screenings at levels broadly consistent with longer term norms. And we are maintaining our focus on getting people the care they need. And acuity patterns remain stable, but as always, we are highly respectful of and watchful for evolving medical cost trends. Looking now at the performance of our specific businesses. OptumHealthâs third quarter revenue increased 34% year-over-year as revenue per consumer grew 31%. Growth continues to be led by the increasing number of patients served under value-based care relationships and the expanding types of care settings offered by Optum from meeting behavioral needs to comprehensively serving people in their homes, to higher acuity ambulatory surgery. OptumInsightâs revenue grew 18% in the quarter, led by continued market growth across payer and provider services. And the revenue backlog increased by $1.8 billion year-over-year to $24.1 billion. OptumRx revenue grew 8%, reflecting growth in people served and continued expansion of the pharmacy care businesses, including specialty, home delivery and community pharmacies. Pharmacy care services revenue growth continues to show momentum growing double-digits in the quarter compared to the prior year. And new customer sales and retention have been strong. Turning to UnitedHealthcare, revenue grew by 11% with all businesses contributing. The number of people served domestically by our commercial insured offerings increased by more than 100,000 over the past half year as we continue to experience strong growth in our newer, more affordable consumer-centric offerings. Products such as Surest, which provides consumers with greater certainty and choice over their health benefits and also our virtual first health offerings. People served by our Medicare Advantage offerings continue to grow strongly, increasing 800,000 so far this year. The recently released 2024 plan year star ratings were consistent with our long-term planning expectations, with 81% of our members in 4-star or better plans, a level we expect will rise as planned refinements are finalized. UnitedHealthcare enters 2023 and serving more people in 4 and 5-star plans than any other health plan. The number of people we serve through our Medicaid offerings has grown by 350,000 year-to-date. Most recently, we were awarded the opportunity to continue to serve the people of Nebraska in TANF, CHIP and long-term care programs. UnitedHealthcare achieved the highest score both overall and in each of the individual categories, reflecting our ability to deliver differentiated solutions aligned to our state customersâ needs. And we continue to see strength in our dual special needs offerings with exceptional consumer satisfaction, demonstrated by a net promoter score of 80. Our capital capacities remain strong. Year-to-date, adjusted cash flows from operations were $21 billion or 1.3x net income. We ended the quarter with a debt-to-capital ratio of 38%, providing ample ability to continue to further build upon vital capabilities, which benefit both the people we serve and the broader health system. And we have returned $10.5 billion to shareholders in the first 9 months of the year through dividends and share repurchases. As noted earlier, given the strength of our business performance this morning, we updated our 2022 adjusted earnings outlook to a range of $21.85 to $22.05 per share. So we think these numbers are telling the story of an enterprise striving to conclude a strong 2022, a year broadly featuring diversified growth today and making foundational investments for our long-term future. Now Iâll turn it back to Andrew.\r\n",
            "Andrew Witty: Thanks, John. As is customary with the close of a third quarter, we will offer early observations about next year. While reserving most of this conversation for our November 29 investor conference. Our businesses are growing and operating well with strong momentum and a keen enterprise focus on executing on our strategic growth priorities. Among a few highlights. The OptumHealth care delivery businesses are rapidly advancing their value-based capacities, expanding the scope and settings of care offered and creating a long runway for growth. And we see our innovative and consistently highly valued Medicare Advantage plans as well positioned to grow strongly again next year. At this distance, we view a majority of the 2023 analyst estimates as reasonably reflecting performance levels we would expect to offer in November with the current consensus at the top end of our likely initial earnings outlook range. And as you have come to expect, we continue to strive toward our long-term 13% to 16% earnings per share growth goal. We look forward to discussing this with you in much greater detail in person at our investor conference in New York. I hope youâre getting a sense of an organization that has long been the case is focused sharply on executing with excellence in all we do, so that we can meet and exceed our commitments to our customers, clinicians, consumers and the communities we serve and, of course, to our employees and to you, our shareholders. With that, operator, letâs open it up for questions, one per caller please.\r\n",
            "Operator: Thank you.  And we will first hear from A.J. Rice of Credit Suisse.\r\n",
            "A.J. Rice: Thanks. Hi, everybody. Thanks for the comments. As you think about â23 and youâre going through the process of pricing discussions with your commercial employer groups and so forth. Is the basic concept â I know, John, you said continue to return to normal utilization. Is the basic view is that we would project a medical cost trend off of this year thatâs more in that traditional 4% or 5% range mid-single digits. And weâve heard some discussion about maybe there being an unusual bump up in addition to the medical â traditional medical cost trend to allocate something for providers dealing with their labor issues. Is your discussion with employers, is that something youâre seeing getting put in place?\r\n",
            "Andrew Witty: So A.J., thanks so much for the question. And obviously, a key aspect of planning for next year that youâre focused on. Just let me make a couple of broader comments and I might just ask Brian Thompson to reflect a little bit more detail on that as well from the UnitedHealthcare perspective. Obviously, for the last couple of years, there is been a lot of focus on the effect of COVID as you think about impacts on medical cost trend. I think as weâve rolled through this year, honestly, I think itâs become much less about COVID. There is now I think there is a blend of possibly a little bit of COVID effect in the system, but cost of living effects, things like inflation, things like capacity constraints in the system as the labor market tightness has affected different parts of the system at different moments. So I think this whole issue has become actually more complicated in some ways because there is more influences on what you need to think through going forward. With that maybe sort of high-level perspective, maybe, Brian, you might just go a little deeper in terms of how youâve been translating some of that as youâve been thoughtful about pricing going forward.\r\n",
            "Brian Thompson: Sure, Andrew. Thank you. And thank you for the question, A.J. Brian Thompson here. As we do look forward, I would say that we have planned and price for our cost trend a little higher than historical norms. And I think to some of the points that Andrew made, some of them are pretty obvious and inevitable. The first just being the reality that 2022 is a lower starting point. In addition to that, contributing is labor and inflationary costs. And just being respectful of what we donât know, again, probably the pacing of care patterns and how they return to normal, certainly included in that. I would certainly suggest though at the same time, weâre working really hard to manage down the impact of these trends on both our consumers and employers alike. And you mentioned how weâre negotiating with our provider partners. I would say weâve really seen emergence of value-based arrangements at a faster pace than perhaps historically. So weâre not just seeing unit costs accelerate at a higher rate. And in addition, just more product designs that are meaningfully lowering those price points for end consumers, whether itâs virtually enriched products, we mentioned that a little bit. Weâve talked a lot about surest and how that consumer choice model is really driving not only a lower price point. But first dollar coverage is really emerging as a priority for those folks that we support and serve every day. So those are some elements and dynamics not only in the product side, but how weâre looking to our forward view of costs.\r\n",
            "Andrew Witty: Yes. Brian, thanks so much. And I think, A.J., one of the key points, Iâm sure youâll have taken away from what Brian is saying there is, look, there is a degree of â there is some unavoidable pressures in the macro environment that everybody is well aware of but weâre really doing something about that on behalf of our members and our clients and our customers. Our focus is to bring innovation into the marketplace, whether thatâs through the way we design plans like surest in UnitedHealthcare, the way we develop the value-based platform in OptumCare, all of which are designed develop to deliver high-quality care at more affordable levels for folks and then to ensure that, that is something that can rely on year after year after year. So a stability of confidence in the healthcare that they can come to expect. So thatâs how weâre really viewing this. Itâs obviously a dynamic environment, but weâre extremely focused, as you can tell, on making sure we have strong responses to it. Thanks, A.J. And next question please.\r\n",
            "Operator: Next, we will hear from Josh Raskin of Nephron Research.\r\n",
            "Josh Raskin: Hi, thanks. Good morning. Can you refresh us on the components of OptumHealth and sort of how much of that revenue is now coming from OptumCare? And then how many of those consumers are within OptumHealth or actually OptumCare consumers and maybe how many are in some form of risk arrangement and then how many are full capitation? I think youâve given some of those numbers just looking for a refresh.\r\n",
            "Andrew Witty: Yes, Josh, good to hear your voice, and thanks so much for the question. So a very good, very strong quarter, continued strong performance from OptumHealth overall. Iâll pass in a second to Dr. Wyatt Decker to give you a little more flavor on that. Just let me pick out one of those aspects, which is a question you asked about the fully capitated lives within there. So if you think about OptumCare, obviously, a subset of OptumHealth has about 20 million or so folks who we look after through that platform. Probably, youâre still a little under 15% of that number in fully capitation arrangements. So that really tells you â despite the very strong impact thatâs having, you can see the way thatâs reflected through this sustain 34% growth in average revenue per consumer served in OptumHealth. A lot of that is influenced by the capitation shift, but itâs still a really small fraction of the total number of lives that we look after in that part of our organization. So a lot of runway there, and thatâs what really underpins our long-term confidence in sustained growth particularly in OptumCare, which is a key piece of OptumHealth and maybe now pass to Wyatt to give you a little bit more context of how it all fits together, Wyatt.\r\n",
            "Wyatt Decker: Yes. Thanks, Andrew, and thank you, Josh, for the question. Absolutely correct is the importance of capitated and value-based care to both our strategic business model to continue to offer comprehensive healthcare solutions to our nation citizens as well as to our optimistic outlook on growth. Today, over two-thirds of our revenue are derived from value-based care constructs, and that will continue to grow. As Andrew pointed out, we have ample pathways for growth. Youâll see us going deeper and broader in the markets that we serve today as well as going into new markets like the Pacific Northwest and the Northeast. And what I am excited about is increasingly â weâre bringing together the platforms that we have talked about historically and that Dirk touched on today. So think of home and community is helping us bring value-based constructs to a broader set of populations and servicing more comprehensively those we serve in OptumCare. So youâll hear me talking a lot about OptumHealth and our capitated and value-based risk labs because increasingly, weâre bringing all of the services, behavioral, virtual, financial as well as clinics to bear to grow our value-based population. Thank you.\r\n",
            "Andrew Witty: Thank you, Wyatt. Josh thanks so much. Next question please, April.\r\n",
            "Operator: Next, we will hear from Nathan Rich of Goldman Sachs.\r\n",
            "Nathan Rich: Hi, good morning. Thanks for the question. I think non-COVID utilization has been running a bit below baseline this year, but you mentioned kind of care pattern is continuing to normalize. How are you thinking about that concept of normalization for both the fourth quarter? And can you maybe just talk about the factors that you think could have the biggest impact on the recovery in non-COVID utilization volumes as we get into next year?\r\n",
            "Andrew Witty: Yes. Listen, Nathan, thanks for the question. As I said a little earlier, I think a good way to think about this is what has historically been a COVID narrative becomes much more a blended narrative around things like capacity constraints in the system, the cost-of-living folksâ ability to â or desire to access the system right now, which is an area we are working super hard to try and ensure is sustained. Plus, of course, volatility in things like COVID and flu, particularly as you go through Q4, Q1 over the next few months. So I think all of that is essentially in the mix. Weâre taking, I think, a reasonably balanced view of how this is going to play out. Weâve seen now for the bulk of this year, a reasonably stable pattern of care utilization across the portfolio. We are taking a normal year view for flu. So weâre not particularly staking ourselves out to say flu is either going to be very low, as weâve seen in the last couple of years, all very high. We just donât know. There is really no evidence to support any decision at this point in time. So weâve taken a very balanced view as we look forward into flu, which, as you well know, can affect Q4 or Q1 differentially year-to-year. And thatâs essentially how weâre tackling all of this. We feel very confident about the way weâre planning for the next quarter and particularly as we roll into next year. And obviously, we will course correct as things like flu reveal themselves to us. Thanks for the question. Next question.\r\n",
            "Operator: Justin Lake, Wolfe Research.\r\n",
            "Justin Lake: Thanks. Good morning. A couple of membership questions. First, I would love to hear your thoughts on the potential for what happens with Medicaid membership in your mind post redeterminations kind of starting? And then any kind of early view on Medicare Advantage, given the open enrollment kicked in here, what do you think for 2023 in terms of the market? Thanks.\r\n",
            "Andrew Witty: Thanks, Justin. So Iâll ask Tim Spilker and then Tim Noel to comment in a second. Just a couple of kind of introductory thoughts really on your questions, as I said in my prepared comments at the beginning of the call, Justin, redeterminations is a really important potential issue for next year and is something that, as an organization, we are going to â we are really leaning into. Weâre very focused on, and thatâs because weâre concerned that through a redetermination cycle during â23 and â24, depending on when the public health emergency comes to an end, could lead to a situation where folks get dislodged from their coverage. And that would be a huge setback in terms of the progress thatâs been made over the last many years to extend cover. So we are really focused on how we can ensure that people are retained coverage. Tim will talk in a second around some of the ways in which we feel confident we can help but it is a really important area, and itâs an area where we hope the entire industry and its participants all lean in to make sure that people donât get lost in the system as things go through a redetermination cycle. So thatâs going to be a super important area. Let me ask Tim Spilker to go a little deeper on that and then pass straight to Noel to talk about open enrollment. Go ahead.\r\n",
            "Tim Spilker: Yes. Thanks for the question. So first, maybe just some of the numbers, so weâre assuming the PHE will end in January and redeterminations will resume in Q1. And we will share more detail in November around the specifics, but weâre very respectful of a variety of factors that are in play. Certainly, the pacing that will vary by state and then, of course, how consumers respond and behave in terms of the change. So as Andrew mentioned, weâre working hard with our customers. This will be a big lift for states, a really long-term effort over the course of 12 to 14 months. So weâre trying to do our part through data sharing, through outreach to consumers, engaging communities, engaging providers and then really connecting with individuals where they access care, so places like pharmacies. So weâre proud of that work, and weâre proud of our ability to be able to support members as they go through this change. With that, Iâll hand it over to Tim.\r\n",
            "Tim Noel: Yes, good morning. Justin thanks for the question. Tim Noel here. So first, if I may start just with kind of an end cap on 2022, John mentioned in the opening remarks that right now, weâve grown about 800,000 Medicare lives and thatâs consistent with the guidance we gave at our investor conference last year and we are on pace to end this year â22 with full year growth of 900,000 lives. So feel good about the way our value proposition has resonated in â22 and feel like that momentum will head into 2023. So, great feedback from the broker community around our product positioning, how we are investing, our emphasis on investments in the most utilized benefits like drugs and like dental benefits. So, feel very good heading into AEP tomorrow. With respect to the industry, over the long-term basis, we have kind of guided to 8% to 9% Medicare Advantage industry growth. I donât have any reason to see it differently from this vantage point for 2023. And once again, I like my chances to outperform the industry in â23 and have share gaining growth.\r\n",
            "Andrew Witty: Thanks Tim. Next question please.\r\n",
            "Operator: Stephen Baxter of Wells Fargo.\r\n",
            "Stephen Baxter: Yes. Hi. Thanks. Wanted to ask about OptumInsight, as you work to grow this business and add new anchor partners, I would love to hear what you are hearing from your health system customers given the challenges they are facing. How is the pipeline developing? Can you remind us, I guess how much revenue coverage you have for next year at this point? And then just lastly, any update on expectations for change accretion post the divestiture of claims expense. Thank you.\r\n",
            "Andrew Witty: Hi Stephen, thanks so much for the question. I am going to ask Dan Schumacher to respond to the first part of that question, and then John Rex will pick up the point on the change accretion piece. So Dan, go ahead.\r\n",
            "Dan Schumacher: Sure. Thank you, Stephen. Appreciate the question. Obviously, health system partners are under a lot of pressure. And we talked about some of them from the macroeconomic backdrop in terms of shifting side of care and labor shortages, wage inflation, things like that. So, as we engage with the market, we find that health systems are very responsive because we present an opportunity to be able to address some of those short-term needs, but then really importantly, on the mid to long-term, we become a key accelerant in some of their transformation initiatives, things like their preparedness for value-based care as an example, or how they engage digitally with patients. And so those are some of the areas that we have been expanding our reach. We have a robust pipeline, and that pipeline has been growing and conversations have been advancing. And whatâs been really encouraging is from our existing relationships, we have been able to drive really strong outcomes, and that gives us confidence around the performance. And so we are excited to see how that develops over time and that will continue to be a contributor. In terms of â our backlog, it represents about a quarter of our backlog and not surprisingly less in terms of current revenue contribution. And as it relates to coverage into next year, we have well north of half coverage on revenue as we look into next year.\r\n",
            "Andrew Witty: Thanks Dan. John?\r\n",
            "John Rex: Yes. On accretion for change, so yes, it will be accretive next year. In terms of the magnitude of that, really, the important factors and considerations are when it closes within the year, certain expectations in terms of the integration costs that we will â and investments we will like to make in the business and when we get that done earlier in the year versus later in the year for a close and then what it has in terms of impact on the out year an important consideration as we bring that in, I think a full out year view. Also, as I am sure you are very aware, the divestiture that occurred in terms of â with the closing also having an impact on the magnitude of that. So, two important things there. Maybe just actually knowing that you have got models to do here also and as you think about it, 4Q and how that might play in the 4Q. So, we wouldnât expect a change to be additive to OptumInsightâs operating earnings in the fourth quarter. A few elements on that. One, the second half for changes. I know you understand well, is itâs lower half seasonally in terms of earnings. First half typically considerably stronger than the second half. So, the impact there and how that comes in. Clearly, there will be transaction costs that we will be incurring here. So, really, as you think about your modeling purposes, roughly in the zone of $800 million of revenue coming in to the OptumInsight segment without any impact on operating earnings. And of course, coming back on my comments is how we think about impact going forward. We will look to the point â to the extent possible, do any acceleration on other important integration activities. So, we can bring the potential benefits of the combination of OptumInsight and change to the health system more quickly, could see that potentially in the over $100 million in terms of incremental cost that we might look to do and pull that ahead. But all of that is incorporated in the 2020 â in the outlook that we provided this morning, the â22 outlook and our â23 observations also. Thanks.\r\n",
            "Andrew Witty: Thanks John. And I think John tees up a really important point that I just want to reemphasize, which is within the guidance we have given you today for the closeout of â22 and â23, not only are we anticipating obviously continued very strong growth in revenues and the consequential flow through the business. But you are also seeing us create the space to make sure we can make the right investments. So, whether thatâs in making sure we get the integration of change done as promptly as we can, whether itâs how we invest in our consumer capabilities, which really started to get going in Q3 and will continue through Q4 and beyond or whether itâs increasing our investment in our employees to make sure that we are responsive to the cost of living pressures. So, really important that you see that and very much taken into consideration as we thought through our work going forward. The other thing I would just also add is that I donât want to â I think you all understand how important the change integration is for OptumInsight. This is â itâs a great moment to bring together tremendous complementary skills, capabilities, technologies, perspectives on the marketplace. And so as we roll through the next two quarters or three quarters, we really anticipate a kind of new OptumInsight emerging from this integration. And we are very excited to have Neil de Crescenzo as well as Dan Schumacher working together to lead these two organizations as this work goes on. So, a lot of energy potentially being released in this space. And as John said, we have incorporated into our guidance points the space to potentially make the right investments going forward. Next question?\r\n",
            "Operator: Scott Fidel of Stephens.\r\n",
            "Scott Fidel: Hi. Thanks. Good morning. I know that continuity of care has been a big focus here, especially with redeterminations coming back. And just interested in that context how you are thinking about potential footprint expansions in the ACA exchanges for 2023 and beyond. It looks like also there has been some favorable policy developments and then some competitive changes to that could be positive for the market. So, just interested in your thinking on exchange strategy for both next year and longer term. Thanks.\r\n",
            "Andrew Witty: I really appreciate that question. As you know, this is an area where we have been building up very substantially over the last couple or 3 years and maybe ask Brian to give you a little bit more detail on that.\r\n",
            "Brian Thompson: Sure Andrew. Thanks for the question, Scott. And we agree totally. We see the exchanges is really emerging as a meaningful place and a broader coverage criteria across this country. And itâs certainly important to us to be more relevant each and every year. In fact, while we are expanding into four new states in 2023, our footprint is expanding meaningfully. I think we are going from around 40% of the addressable market here, leaving 2022 to nearly two-thirds by the end of next year and again â or by the start of next year, I should suggest. So, again, reasons aligned with yours. Itâs important that we are where folks are. And as we manage through this redetermination process working not only with our employer partners or distribution partners in our states, but making sure we have product ourselves to make sure that these folks can get enrolled in. And we are encouraged by our progress. We like how we are positioned here, leaving 2022 both from a footprint and receptivity and growth and a performance perspective and looking forward to expanding again in â23.\r\n",
            "Andrew Witty: Great. Thanks so much, Brian. Thank you for that question. Next question please.\r\n",
            "Operator: Kevin Fischbeck of Bank of America.\r\n",
            "Kevin Fischbeck: Great. Thanks. I want to go back to trend, again, if I can. Can you give a little more color in the quarter about how trends looked across the three different payers? And when you talk about normalization in 2023, should we expect all three of them to be kind of back to normal, or do you have a view that the government, which has been lagging a little bit, will still kind of take a little bit longer to get back to normal. Thanks.\r\n",
            "Andrew Witty: Kevin, thanks so much for the question. Maybe I will ask Brian to make a couple of comments on that.\r\n",
            "Brian Thompson: Sure. Hi. Thanks for the question, Kevin. Maybe I will start with service type, and itâs really a focus on inpatient, that has been the driver, and it really was the driver last quarter as well. So, when I look at third quarter, itâs largely a repeat of what we saw in the second quarter. Beyond inpatient, some variation, I would say, all other service types to largely near at or even slightly above normal levels. To your question, though, on lines of business, I might just suggest other than inpatient commercial, pretty much at normal care patterns. Medicare is seeing some interesting developments. I would say maybe signs of more durable shifts inside of service, particularly urgent care is a little lower, excuse me, a little higher, but ER is a little lower, and thatâs certainly a good trade for the system overall. Outpatient surgeries in Medicare seem to be back to normal. But again, as I had mentioned, inpatient a little bit lower. And again, Medicaid, consistent with the other lines of business, lower in inpatient and physicians starting to trend back. So, again, thatâs largely a repeat of what I had suggested last quarter inpatient really being the most notable element inside it.\r\n",
            "Andrew Witty: Brian, thanks so much, and thank you for the question Kevin. Next question please.\r\n",
            "Operator: Next we have from Lisa Gill of JPMorgan.\r\n",
            "Lisa Gill: Thanks very much for taking my question. When we think about membership in the commercial market, I am just curious how employers are currently talking to you about that trend going into â23. And if you can just give us an update on your thoughts around if we move into a recessionary type of environment what that could mean for the health plan business?\r\n",
            "Andrew Witty: Lisa, thanks so much for the question. And I will pass that actually to our new Head of ENI, our commercial insurance business, Dan Keuter. Dan, would you like to answer that?\r\n",
            "Dan Keuter: Yes. Thanks Andrew for the introduction. And Lisa thanks for the question. As you know, at this point in the year, our national accounts business for 2023 is largely resolved, but the other segments of our business are not resolved. As we look at that national accounts performance, we are very pleased with what we have seen in terms of a very strong renewal year and also a strong sales year, which leaves us I am confident in a growth year for national accounts for 2023. The other lines of the business are yet to resolve. Specifically related to your question about recessionary impacts, we are certainly well aware of those. However, I would note at this point, based on our Q3 performance, we have seen net hiring among our customers. So, we have not yet seen an emergence of recessionary impact in our commercial book of business. Looking forward to 2023, we will assess the continued evolving economic situation and provide additional guidance as that becomes more clear. Thanks for your question, Lisa.\r\n",
            "Andrew Witty: Dan, thank you very much for that and Lisa, thank you for the question. So, last question, please, operator.\r\n",
            "Operator: Absolutely. Our final question for today will come from Gary Taylor of Cowen.\r\n",
            "Gary Taylor: Hi. Good morning. I just want to come back to â23 for a moment. I think the current Street consensus is about 13% earnings growth off your updated â22 guidance. And I was hoping maybe this would be a year where you wouldnât have to say initially that look to be at the high end of the initial range. And obviously, I understand guidance has often come up over the course of the year. But when I think about â23, I just want to make sure I am capturing what you are saying. You talked about redeterminations. You talked about some incremental investments in consumer and employees. And you also talked about, I think, just sort of healthy respect, as you might call it, for maybe utilization continuing to normalize. Could you rank those for us in terms of how you are thinking those headwinds weigh on â23 a little bit? Is there anything else you would say is material enough to add to that list?\r\n",
            "Andrew Witty: Gary, thanks so much for the questions. I mean if you kind of just think about the blend of headwinds, tailwinds for 2023, in terms of headwinds, I would probably say the external environment, the inflationary pressures, obviously, in among that mix. Question on whether or not we see any kind of economic slowdown as you just heard, we are not really seeing that yet in terms of our marketplace, but we have got to be thoughtful about that, and we want to make sure that we go into the year, not assuming anything overly optimistic. It would be great to have pleasant surprises on that dimension, of course, utilization levels, I spent a lot of time on this call talking about that as we start to see that continue to normalize. And then the investments, I mean those are really the four areas of investments in our business and you recited those very nicely. Now on the other side, you think about the tailwinds for the business. Very strong income and momentum for the organization. OptumHealthâs growth in patients served, the evolution of the at-home platform, which has got over the last 2.5 years, 3 years, has moved to a very significant scale, complementing our clinic platform, strong capabilities that we are demonstrating in Medicare Advantage. You look at those really driving tailwinds for the organization, very substantial. The scale of those organizations and what we are able to do very material. And then I would also say our ability to continue to deploy capital effectively in the marketplace is another one of the things to really think about. And itâs interesting, actually, just to pause for a second today and contrast where we are today versus where we were a year ago. So, year-to-date 2021, we have deployed about $5 billion of capital in M&A, which was about the full year number in the end. As of today, we have spent $20 billion in 2022. So, in terms of our ramp-up of capital deployment, and you heard from John Rex earlier, we have very substantial continued capacity. And obviously, the marketplace is getting interesting around that space. We have talked several times to you in the past about the diverse pipeline of opportunities that we see probably as diverse as we have ever seen. Very much opportunities we see across a number of our key growth platforms. And as we all know, the market is beginning to be very discriminating in terms of value. So, we will see how that plays out for us, but I think you have to expect that to be a tailwind for us as well going into next year. With that, Gary, thanks so much for the question. And thanks, everybody, for your time and questions today. We do hope you are taking away the impression of a company confident in its opportunities and ability to grow. Deeply aware of where and how we need to continue to build and improve and fully committed to our mission of helping people live healthier lives and helping make the health system work better for everyone. I want to thank you for your attention and we look forward to meeting many of you in person in New York later in the quarter. Thank you.\r\n",
            "End of Q&A:\r\n",
            "Operator: And that does conclude todayâs conference. Thank you all for your participation. You may now disconnect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a pretrained w2v-d2v model\n",
        "ec2vec = Doc2Vec.load('/content/dlta_ec2vec/ec_d2v_without_docvecs.pickle')\n",
        "\n",
        "# load a dataframe which contains all words known to the model\n",
        "with open('/content/dlta_ec2vec/ec_d2v_dict.pickle', 'rb') as handle:\n",
        "    known_words = pickle.load(handle)"
      ],
      "metadata": {
        "id": "mBO6v9buYmEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "known_words.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "AmH-HtmKCvka",
        "outputId": "02ae6b53-db8c-42d1-cc3e-927bc9fbad74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      word_id    count\n",
              "the         0  5352601\n",
              "and         1  3247170\n",
              "that        2  3187856\n",
              "you         3  2053317\n",
              "for         4   937915"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-805a4f23-4802-42c1-bc88-db5b53cacf83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_id</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0</td>\n",
              "      <td>5352601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>1</td>\n",
              "      <td>3247170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>2</td>\n",
              "      <td>3187856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>you</th>\n",
              "      <td>3</td>\n",
              "      <td>2053317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>4</td>\n",
              "      <td>937915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-805a4f23-4802-42c1-bc88-db5b53cacf83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-805a4f23-4802-42c1-bc88-db5b53cacf83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-805a4f23-4802-42c1-bc88-db5b53cacf83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"expect\" in known_words.index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWSrOOg2EWQI",
        "outputId": "a7b345e6-ec65-488b-958a-213dd60bc137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(ec2vec.wv.key_to_index.keys())\n",
        "\n",
        "words = []\n",
        "for word in word_list:\n",
        "  if word in vocab:\n",
        "    words.append(word)\n",
        "print(len(word_list))\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h406ttA57Igv",
        "outputId": "3e9cdd8c-ae3c-46f4-97d2-8f00eba675d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "473\n",
            "406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "import numpy as np\n",
        "\n",
        "# a custom error for q and a retrieval\n",
        "class AnalystError(Exception):\n",
        "    def __init__(self):\n",
        "        super().__init__('No analysts found in the earning call, it is not possible to identify the start of questioning, moving to the next!')\n",
        "    \n",
        "# a function to retrieve questions and answers in earning calls\n",
        "def get_q_and_a(ec_transcript, min_token = 5, max_token = 500, raw_text = False):\n",
        "\n",
        "    if raw_text == True:\n",
        "        # temporary empty df to collect questions and answers, only\n",
        "        ec_docs = pd.DataFrame(columns = ['name', 'doc_tokens', 'raw_text'])\n",
        "    else:\n",
        "        # temporary empty df to collect questions and answers, only\n",
        "        ec_docs = pd.DataFrame(columns = ['name', 'doc_tokens'])\n",
        "\n",
        "    # determine ec participants\n",
        "    ec_participants = {}\n",
        "    # to append rows, initialize a running index which is set to zero for every ec_transcript\n",
        "    local_id = 0\n",
        "    for par in ec_transcript.lower().split('\\n'):\n",
        "        name, text = par.split(':', maxsplit = 1)\n",
        "        name = ' '.join(simple_preprocess(name))\n",
        "        text2tokens = simple_preprocess(text)\n",
        "        text2tokens = [token for token in text2tokens if len(token) > 2]\n",
        "        text2string = ','.join(text2tokens) \n",
        "\n",
        "        # we track and use only documents in a certain range of words used, this only reduces the amount of questions and answers by approx. 1%\n",
        "        if (min_token < len(text2tokens) <= max_token) & (len(name) > 0) & (name != 'operator'):\n",
        "\n",
        "            ec_docs.loc[local_id, 'name'] = name\n",
        "            ec_docs.loc[local_id, 'doc_tokens'] = text2string\n",
        "\n",
        "            if raw_text == True:\n",
        "                ec_docs.loc[local_id, 'raw_text'] = text\n",
        "            \n",
        "            local_id += 1\n",
        "\n",
        "            words_spoken = len(text2tokens)\n",
        "            if name in ec_participants.keys():\n",
        "                ec_participants[name].append(words_spoken)\n",
        "            else:\n",
        "                ec_participants[name] = [] \n",
        "                ec_participants[name].append(words_spoken)   \n",
        "\n",
        "    # get speaker statistics\n",
        "    ec_summary = {\n",
        "        'nbr_occurr': [],\n",
        "        'max_words': [],\n",
        "        'avg_words': []\n",
        "    }\n",
        "\n",
        "    for key in ec_participants.keys():\n",
        "        ec_summary['nbr_occurr'].append(len(ec_participants[key]))\n",
        "        ec_summary['max_words'].append(np.max(ec_participants[key]))\n",
        "        ec_summary['avg_words'].append(np.mean(ec_participants[key]))\n",
        "\n",
        "    ec_summary = pd.DataFrame(ec_summary, index = ec_participants.keys())\n",
        "    ec_summary.sort_values(by = 'nbr_occurr', ascending=False, inplace=True)\n",
        "    ec_summary.loc[:, 'frac_occur'] = ec_summary.nbr_occurr / ec_summary.nbr_occurr.sum()\n",
        "\n",
        "    ### identify firm representatives and analysts\n",
        "\n",
        "    # very likely the operator and firm officials will speak more often\n",
        "    firm_people = list(ec_summary[ec_summary.frac_occur > .10].index.values)\n",
        "    # analysts usually do not ask more than about 2,3 or four times, we assume that no analysts talks more than 10% of all spoken paragraphs\n",
        "    analysts = list(ec_summary[ec_summary.frac_occur <= .10].index.values)\n",
        "\n",
        "    # sometimes officicals speak less, but may give a statement in the beginning, this is how we try to capture this\n",
        "    officials_lost = list(set(list(ec_participants.keys())[:2]).intersection(set(analysts)))\n",
        "    if len(officials_lost) > 0:\n",
        "        for official in officials_lost:\n",
        "            firm_people.append(official)\n",
        "            analysts.remove(official)\n",
        "\n",
        "    try:\n",
        "        # extract questions and answers by checking when the first time an analyst starts reading\n",
        "        fst_occurrence_of_analyst = [(name in analysts) for name in ec_docs.name.to_list()].index(True)\n",
        "        q_and_a_part = ec_docs.iloc[fst_occurrence_of_analyst:, :]\n",
        "        \n",
        "        return q_and_a_part\n",
        "    except:\n",
        "        raise AnalystError"
      ],
      "metadata": {
        "id": "1BL-cABUdaYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_and_answers = get_q_and_a(transcripts.content[10], raw_text = True)\n",
        "questions_and_answers.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tCgDjFRRddMi",
        "outputId": "64d47daf-85d7-4efa-9512-a1970684c67d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               name                                         doc_tokens  \\\n",
              "4  christine arnold  good,morning,your,pdp,medicare,enrolment,are,a...   \n",
              "5   stephen hemsley  well,would,point,out,that,january,and,let,then...   \n",
              "6     carl mcdonald  great,thank,you,some,the,smaller,private,plans...   \n",
              "7   stephen hemsley  maybe,they,should,answer,that,kind,enterprise,...   \n",
              "8        mike mikan  you,know,really,haven,seen,any,real,change,und...   \n",
              "\n",
              "                                            raw_text  \n",
              "4   good morning. your pdp in medicare enrolment ...  \n",
              "5   well, i would point out that it is january, a...  \n",
              "6   great. thank you. some of the smaller private...  \n",
              "7   no, maybe they should answer that kind of in ...  \n",
              "8   you know, we really haven't seen any real cha...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f47c757-e397-42b1-b01b-6fe6dabffad6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>doc_tokens</th>\n",
              "      <th>raw_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>christine arnold</td>\n",
              "      <td>good,morning,your,pdp,medicare,enrolment,are,a...</td>\n",
              "      <td>good morning. your pdp in medicare enrolment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>stephen hemsley</td>\n",
              "      <td>well,would,point,out,that,january,and,let,then...</td>\n",
              "      <td>well, i would point out that it is january, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>carl mcdonald</td>\n",
              "      <td>great,thank,you,some,the,smaller,private,plans...</td>\n",
              "      <td>great. thank you. some of the smaller private...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>stephen hemsley</td>\n",
              "      <td>maybe,they,should,answer,that,kind,enterprise,...</td>\n",
              "      <td>no, maybe they should answer that kind of in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mike mikan</td>\n",
              "      <td>you,know,really,haven,seen,any,real,change,und...</td>\n",
              "      <td>you know, we really haven't seen any real cha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f47c757-e397-42b1-b01b-6fe6dabffad6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f47c757-e397-42b1-b01b-6fe6dabffad6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f47c757-e397-42b1-b01b-6fe6dabffad6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "for this task, we have to find th 5 closest documents to each document. In the *top5_similar_docs* dataframe, we can see these documents and their cosine similarity. The first rows are very close to each other but contain no important information and are almost part of all calls. they only focus on thanking someone and asking for the next question. These have nothing to do with the topic of the call. After these, we see a lot of documents asking someone else to answer the question, add to the answer or Thanking the analyst for asking a good question."
      ],
      "metadata": {
        "id": "iAXE0LYujZkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "docvecs = []\n",
        "raw_docs = []\n",
        "\n",
        "for ec in transcripts.content:\n",
        "\n",
        "  questions_and_answers = get_q_and_a(ec, raw_text = True)\n",
        "\n",
        "  for doc, raw_doc in zip(questions_and_answers.doc_tokens.values, questions_and_answers.raw_text.values):\n",
        "    docvecs.append(ec2vec.infer_vector(doc.split(',')))\n",
        "    raw_docs.append(raw_doc)\n",
        "  \n",
        "docvecs = np.array(docvecs)\n",
        "norm_docvecs = normalize(docvecs)\n",
        "doc_res = np.inner(norm_docvecs, norm_docvecs)\n",
        "top_docs = np.flip(np.argsort(doc_res, axis=1), axis=1)[:, 1:]\n",
        "top_scores = np.flip(np.sort(doc_res, axis=1), axis=1)[:, 1:]\n",
        "\n",
        "docs_with_highest_similiarity = np.flip(np.argsort(top_scores[:, 0]))"
      ],
      "metadata": {
        "id": "VllmsqLQdn8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs_with_highest_similiarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7GHlBfUxWNr",
        "outputId": "c8e24bc5-7f90-4179-da27-5734306fbf4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3722"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top5_similar_docs = pd.DataFrame(columns=['original','first doc','first doc cosine similarity','second doc','second doc cosine similarity',\n",
        "                                          'third doc','third doc cosine similarity','forth doc','forth doc cosine similarity','fifth doc','fifth doc cosine similarity'])\n",
        "for doc_id in docs_with_highest_similiarity:\n",
        "  top5_similar_docs = top5_similar_docs.append({'original':raw_docs[doc_id]\n",
        "                            ,'first doc':raw_docs[top_docs[doc_id][0]],'first doc cosine similarity':top_scores[doc_id][0]\n",
        "                            ,'second doc':raw_docs[top_docs[doc_id][1]],'second doc cosine similarity':top_scores[doc_id][1]\n",
        "                            ,'third doc':raw_docs[top_docs[doc_id][2]],'third doc cosine similarity':top_scores[doc_id][2]\n",
        "                            ,'forth doc':raw_docs[top_docs[doc_id][3]],'forth doc cosine similarity':top_scores[doc_id][3]\n",
        "                            ,'fifth doc':raw_docs[top_docs[doc_id][4]],'fifth doc cosine similarity':top_scores[doc_id][4]\n",
        "                            }, ignore_index=True)\n",
        "  "
      ],
      "metadata": {
        "id": "ZK3sQfudeF5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top5_similar_docs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "D4BKjUI50mJ8",
        "outputId": "a6729c66-143f-4f20-d686-a8320a4c6745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            original  \\\n",
              "0          thank you, lance. next question please.\\r   \n",
              "1          thank you, lance. next question please?\\r   \n",
              "2          sure, i think gail can respond to this.\\r   \n",
              "3         i think this one we can respond to gail.\\r   \n",
              "4   great. thank you, lance. next question, pleas...   \n",
              "\n",
              "                                     first doc  first doc cosine similarity  \\\n",
              "0    thank you, lance. next question please?\\r                     0.998295   \n",
              "1    thank you, lance. next question please.\\r                     0.998295   \n",
              "2   i think this one we can respond to gail.\\r                     0.997445   \n",
              "3    sure, i think gail can respond to this.\\r                     0.997445   \n",
              "4    thank you, lance. next question please?\\r                     0.997360   \n",
              "\n",
              "                                          second doc  \\\n",
              "0   thank you for the question, lance. next quest...   \n",
              "1   great. thank you, lance. next question, pleas...   \n",
              "2   i think gail is probably best to respond to t...   \n",
              "3   i think gail is probably best to respond to t...   \n",
              "4   thank you for the question, lance. next quest...   \n",
              "\n",
              "   second doc cosine similarity  \\\n",
              "0                      0.995184   \n",
              "1                      0.997360   \n",
              "2                      0.992084   \n",
              "3                      0.993117   \n",
              "4                      0.994819   \n",
              "\n",
              "                                           third doc  \\\n",
              "0   great. thank you, lance. next question, pleas...   \n",
              "1   thank you for the question, lance. next quest...   \n",
              "2   we will. and now keep in mind we have a very ...   \n",
              "3   we will. and now keep in mind we have a very ...   \n",
              "4          thank you, lance. next question please.\\r   \n",
              "\n",
              "   third doc cosine similarity  \\\n",
              "0                     0.994723   \n",
              "1                     0.995841   \n",
              "2                     0.831682   \n",
              "3                     0.828170   \n",
              "4                     0.994723   \n",
              "\n",
              "                                           forth doc  \\\n",
              "0   great question lance. appreciate it. john pri...   \n",
              "1   great question lance. appreciate it. john pri...   \n",
              "2   that's quite a question. gail, do you want to...   \n",
              "3   that's quite a question. gail, do you want to...   \n",
              "4   great question lance. appreciate it. john pri...   \n",
              "\n",
              "   forth doc cosine similarity  \\\n",
              "0                     0.703684   \n",
              "1                     0.697993   \n",
              "2                     0.817250   \n",
              "3                     0.806017   \n",
              "4                     0.688539   \n",
              "\n",
              "                                           fifth doc  \\\n",
              "0   sure, good morning lance. i would tell you th...   \n",
              "1   sure, good morning lance. i would tell you th...   \n",
              "2   very market-specific, but gail, do you want t...   \n",
              "3   very market-specific, but gail, do you want t...   \n",
              "4   sure, good morning lance. i would tell you th...   \n",
              "\n",
              "   fifth doc cosine similarity  \n",
              "0                     0.571161  \n",
              "1                     0.572635  \n",
              "2                     0.816267  \n",
              "3                     0.804098  \n",
              "4                     0.561083  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a07872c3-e9be-4a2e-b171-f61de1825894\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>first doc</th>\n",
              "      <th>first doc cosine similarity</th>\n",
              "      <th>second doc</th>\n",
              "      <th>second doc cosine similarity</th>\n",
              "      <th>third doc</th>\n",
              "      <th>third doc cosine similarity</th>\n",
              "      <th>forth doc</th>\n",
              "      <th>forth doc cosine similarity</th>\n",
              "      <th>fifth doc</th>\n",
              "      <th>fifth doc cosine similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thank you, lance. next question please.\\r</td>\n",
              "      <td>thank you, lance. next question please?\\r</td>\n",
              "      <td>0.998295</td>\n",
              "      <td>thank you for the question, lance. next quest...</td>\n",
              "      <td>0.995184</td>\n",
              "      <td>great. thank you, lance. next question, pleas...</td>\n",
              "      <td>0.994723</td>\n",
              "      <td>great question lance. appreciate it. john pri...</td>\n",
              "      <td>0.703684</td>\n",
              "      <td>sure, good morning lance. i would tell you th...</td>\n",
              "      <td>0.571161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thank you, lance. next question please?\\r</td>\n",
              "      <td>thank you, lance. next question please.\\r</td>\n",
              "      <td>0.998295</td>\n",
              "      <td>great. thank you, lance. next question, pleas...</td>\n",
              "      <td>0.997360</td>\n",
              "      <td>thank you for the question, lance. next quest...</td>\n",
              "      <td>0.995841</td>\n",
              "      <td>great question lance. appreciate it. john pri...</td>\n",
              "      <td>0.697993</td>\n",
              "      <td>sure, good morning lance. i would tell you th...</td>\n",
              "      <td>0.572635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sure, i think gail can respond to this.\\r</td>\n",
              "      <td>i think this one we can respond to gail.\\r</td>\n",
              "      <td>0.997445</td>\n",
              "      <td>i think gail is probably best to respond to t...</td>\n",
              "      <td>0.992084</td>\n",
              "      <td>we will. and now keep in mind we have a very ...</td>\n",
              "      <td>0.831682</td>\n",
              "      <td>that's quite a question. gail, do you want to...</td>\n",
              "      <td>0.817250</td>\n",
              "      <td>very market-specific, but gail, do you want t...</td>\n",
              "      <td>0.816267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i think this one we can respond to gail.\\r</td>\n",
              "      <td>sure, i think gail can respond to this.\\r</td>\n",
              "      <td>0.997445</td>\n",
              "      <td>i think gail is probably best to respond to t...</td>\n",
              "      <td>0.993117</td>\n",
              "      <td>we will. and now keep in mind we have a very ...</td>\n",
              "      <td>0.828170</td>\n",
              "      <td>that's quite a question. gail, do you want to...</td>\n",
              "      <td>0.806017</td>\n",
              "      <td>very market-specific, but gail, do you want t...</td>\n",
              "      <td>0.804098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great. thank you, lance. next question, pleas...</td>\n",
              "      <td>thank you, lance. next question please?\\r</td>\n",
              "      <td>0.997360</td>\n",
              "      <td>thank you for the question, lance. next quest...</td>\n",
              "      <td>0.994819</td>\n",
              "      <td>thank you, lance. next question please.\\r</td>\n",
              "      <td>0.994723</td>\n",
              "      <td>great question lance. appreciate it. john pri...</td>\n",
              "      <td>0.688539</td>\n",
              "      <td>sure, good morning lance. i would tell you th...</td>\n",
              "      <td>0.561083</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a07872c3-e9be-4a2e-b171-f61de1825894')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a07872c3-e9be-4a2e-b171-f61de1825894 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a07872c3-e9be-4a2e-b171-f61de1825894');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top5_similar_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6517
        },
        "id": "bMTgw195o_CI",
        "outputId": "2d264cfa-69c8-4991-f792-81aaa3c83a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               original  \\\n",
              "0             thank you, lance. next question please.\\r   \n",
              "1             thank you, lance. next question please?\\r   \n",
              "2             sure, i think gail can respond to this.\\r   \n",
              "3            i think this one we can respond to gail.\\r   \n",
              "4      great. thank you, lance. next question, pleas...   \n",
              "...                                                 ...   \n",
              "3717   my believe that most of your writings have be...   \n",
              "3718   good morning. yes, certainly, anything more t...   \n",
              "3719   and steve just a quick follow-up on that, do ...   \n",
              "3720   i mean, it’s kind of hard to ignore the numbe...   \n",
              "3721   sure. thanks steve. matt, i appreciate the qu...   \n",
              "\n",
              "                                              first doc  \\\n",
              "0             thank you, lance. next question please?\\r   \n",
              "1             thank you, lance. next question please.\\r   \n",
              "2            i think this one we can respond to gail.\\r   \n",
              "3             sure, i think gail can respond to this.\\r   \n",
              "4             thank you, lance. next question please?\\r   \n",
              "...                                                 ...   \n",
              "3717   thanks. good morning. there are signals in th...   \n",
              "3718   our expectation is that we'll be in the zone ...   \n",
              "3719   and you're seeing pressure in the usual place...   \n",
              "3720   sure. this is jack larsen. just to answer you...   \n",
              "3721   a little bit of perspective on what’s happeni...   \n",
              "\n",
              "      first doc cosine similarity  \\\n",
              "0                        0.998295   \n",
              "1                        0.998295   \n",
              "2                        0.997445   \n",
              "3                        0.997445   \n",
              "4                        0.997360   \n",
              "...                           ...   \n",
              "3717                     0.278033   \n",
              "3718                     0.276096   \n",
              "3719                     0.275706   \n",
              "3720                     0.275454   \n",
              "3721                     0.255249   \n",
              "\n",
              "                                             second doc  \\\n",
              "0      thank you for the question, lance. next quest...   \n",
              "1      great. thank you, lance. next question, pleas...   \n",
              "2      i think gail is probably best to respond to t...   \n",
              "3      i think gail is probably best to respond to t...   \n",
              "4      thank you for the question, lance. next quest...   \n",
              "...                                                 ...   \n",
              "3717   well, josh, i'm going to keep this broad, but...   \n",
              "3718   yes, hi. thank you. just a follow-up on a pre...   \n",
              "3719   could i see that? scott fidel - deutsche bank...   \n",
              "3720   just turning to medicaid in terms of the expe...   \n",
              "3721   and then, finally, when do you expect during ...   \n",
              "\n",
              "      second doc cosine similarity  \\\n",
              "0                         0.995184   \n",
              "1                         0.997360   \n",
              "2                         0.992084   \n",
              "3                         0.993117   \n",
              "4                         0.994819   \n",
              "...                            ...   \n",
              "3717                      0.271122   \n",
              "3718                      0.274093   \n",
              "3719                      0.273225   \n",
              "3720                      0.273857   \n",
              "3721                      0.252560   \n",
              "\n",
              "                                              third doc  \\\n",
              "0      great. thank you, lance. next question, pleas...   \n",
              "1      thank you for the question, lance. next quest...   \n",
              "2      we will. and now keep in mind we have a very ...   \n",
              "3      we will. and now keep in mind we have a very ...   \n",
              "4             thank you, lance. next question please.\\r   \n",
              "...                                                 ...   \n",
              "3717   i will respond the same way we do when we hav...   \n",
              "3718   hi, good morning. i just had a question. now ...   \n",
              "3719   hi thanks, good morning. i want to talk a lit...   \n",
              "3720   i want to probe medicare advantage and let’s ...   \n",
              "3721   and dcp, thank you for reminding, so dcp is d...   \n",
              "\n",
              "      third doc cosine similarity  \\\n",
              "0                        0.994723   \n",
              "1                        0.995841   \n",
              "2                        0.831682   \n",
              "3                        0.828170   \n",
              "4                        0.994723   \n",
              "...                           ...   \n",
              "3717                     0.262259   \n",
              "3718                     0.242445   \n",
              "3719                     0.269723   \n",
              "3720                     0.271252   \n",
              "3721                     0.251789   \n",
              "\n",
              "                                              forth doc  \\\n",
              "0      great question lance. appreciate it. john pri...   \n",
              "1      great question lance. appreciate it. john pri...   \n",
              "2      that's quite a question. gail, do you want to...   \n",
              "3      that's quite a question. gail, do you want to...   \n",
              "4      great question lance. appreciate it. john pri...   \n",
              "...                                                 ...   \n",
              "3717   we don't know and we really can't offer an op...   \n",
              "3718   maybe i will start out by saying, first of al...   \n",
              "3719   sure. i think the question relates to nationa...   \n",
              "3720   well, it is all about cost and there is you k...   \n",
              "3721   yes. so if we look at, in particular -- so fi...   \n",
              "\n",
              "      forth doc cosine similarity  \\\n",
              "0                        0.703684   \n",
              "1                        0.697993   \n",
              "2                        0.817250   \n",
              "3                        0.806017   \n",
              "4                        0.688539   \n",
              "...                           ...   \n",
              "3717                     0.257579   \n",
              "3718                     0.236070   \n",
              "3719                     0.269054   \n",
              "3720                     0.270057   \n",
              "3721                     0.250678   \n",
              "\n",
              "                                              fifth doc  \\\n",
              "0      sure, good morning lance. i would tell you th...   \n",
              "1      sure, good morning lance. i would tell you th...   \n",
              "2      very market-specific, but gail, do you want t...   \n",
              "3      very market-specific, but gail, do you want t...   \n",
              "4      sure, good morning lance. i would tell you th...   \n",
              "...                                                 ...   \n",
              "3717   great. listen, robert, thanks so much for the...   \n",
              "3718   again, it's a maturing marketplace. it's real...   \n",
              "3719   yes, i think the market is postured for longe...   \n",
              "3720   have the employers been proactivein asking yo...   \n",
              "3721   sure. i will weigh into this and let dave dea...   \n",
              "\n",
              "      fifth doc cosine similarity  \n",
              "0                        0.571161  \n",
              "1                        0.572635  \n",
              "2                        0.816267  \n",
              "3                        0.804098  \n",
              "4                        0.561083  \n",
              "...                           ...  \n",
              "3717                     0.256608  \n",
              "3718                     0.235670  \n",
              "3719                     0.262632  \n",
              "3720                     0.265573  \n",
              "3721                     0.247456  \n",
              "\n",
              "[3722 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4acdaa8-52a8-4eaf-a400-4b3cd644db98\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>first doc</th>\n",
              "      <th>first doc cosine similarity</th>\n",
              "      <th>second doc</th>\n",
              "      <th>second doc cosine similarity</th>\n",
              "      <th>third doc</th>\n",
              "      <th>third doc cosine similarity</th>\n",
              "      <th>forth doc</th>\n",
              "      <th>forth doc cosine similarity</th>\n",
              "      <th>fifth doc</th>\n",
              "      <th>fifth doc cosine similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thank you, lance. next question please.\\r</td>\n",
              "      <td>thank you, lance. next question please?\\r</td>\n",
              "      <td>0.998295</td>\n",
              "      <td>thank you for the question, lance. next quest...</td>\n",
              "      <td>0.995184</td>\n",
              "      <td>great. thank you, lance. next question, pleas...</td>\n",
              "      <td>0.994723</td>\n",
              "      <td>great question lance. appreciate it. john pri...</td>\n",
              "      <td>0.703684</td>\n",
              "      <td>sure, good morning lance. i would tell you th...</td>\n",
              "      <td>0.571161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thank you, lance. next question please?\\r</td>\n",
              "      <td>thank you, lance. next question please.\\r</td>\n",
              "      <td>0.998295</td>\n",
              "      <td>great. thank you, lance. next question, pleas...</td>\n",
              "      <td>0.997360</td>\n",
              "      <td>thank you for the question, lance. next quest...</td>\n",
              "      <td>0.995841</td>\n",
              "      <td>great question lance. appreciate it. john pri...</td>\n",
              "      <td>0.697993</td>\n",
              "      <td>sure, good morning lance. i would tell you th...</td>\n",
              "      <td>0.572635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sure, i think gail can respond to this.\\r</td>\n",
              "      <td>i think this one we can respond to gail.\\r</td>\n",
              "      <td>0.997445</td>\n",
              "      <td>i think gail is probably best to respond to t...</td>\n",
              "      <td>0.992084</td>\n",
              "      <td>we will. and now keep in mind we have a very ...</td>\n",
              "      <td>0.831682</td>\n",
              "      <td>that's quite a question. gail, do you want to...</td>\n",
              "      <td>0.817250</td>\n",
              "      <td>very market-specific, but gail, do you want t...</td>\n",
              "      <td>0.816267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i think this one we can respond to gail.\\r</td>\n",
              "      <td>sure, i think gail can respond to this.\\r</td>\n",
              "      <td>0.997445</td>\n",
              "      <td>i think gail is probably best to respond to t...</td>\n",
              "      <td>0.993117</td>\n",
              "      <td>we will. and now keep in mind we have a very ...</td>\n",
              "      <td>0.828170</td>\n",
              "      <td>that's quite a question. gail, do you want to...</td>\n",
              "      <td>0.806017</td>\n",
              "      <td>very market-specific, but gail, do you want t...</td>\n",
              "      <td>0.804098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great. thank you, lance. next question, pleas...</td>\n",
              "      <td>thank you, lance. next question please?\\r</td>\n",
              "      <td>0.997360</td>\n",
              "      <td>thank you for the question, lance. next quest...</td>\n",
              "      <td>0.994819</td>\n",
              "      <td>thank you, lance. next question please.\\r</td>\n",
              "      <td>0.994723</td>\n",
              "      <td>great question lance. appreciate it. john pri...</td>\n",
              "      <td>0.688539</td>\n",
              "      <td>sure, good morning lance. i would tell you th...</td>\n",
              "      <td>0.561083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3717</th>\n",
              "      <td>my believe that most of your writings have be...</td>\n",
              "      <td>thanks. good morning. there are signals in th...</td>\n",
              "      <td>0.278033</td>\n",
              "      <td>well, josh, i'm going to keep this broad, but...</td>\n",
              "      <td>0.271122</td>\n",
              "      <td>i will respond the same way we do when we hav...</td>\n",
              "      <td>0.262259</td>\n",
              "      <td>we don't know and we really can't offer an op...</td>\n",
              "      <td>0.257579</td>\n",
              "      <td>great. listen, robert, thanks so much for the...</td>\n",
              "      <td>0.256608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3718</th>\n",
              "      <td>good morning. yes, certainly, anything more t...</td>\n",
              "      <td>our expectation is that we'll be in the zone ...</td>\n",
              "      <td>0.276096</td>\n",
              "      <td>yes, hi. thank you. just a follow-up on a pre...</td>\n",
              "      <td>0.274093</td>\n",
              "      <td>hi, good morning. i just had a question. now ...</td>\n",
              "      <td>0.242445</td>\n",
              "      <td>maybe i will start out by saying, first of al...</td>\n",
              "      <td>0.236070</td>\n",
              "      <td>again, it's a maturing marketplace. it's real...</td>\n",
              "      <td>0.235670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3719</th>\n",
              "      <td>and steve just a quick follow-up on that, do ...</td>\n",
              "      <td>and you're seeing pressure in the usual place...</td>\n",
              "      <td>0.275706</td>\n",
              "      <td>could i see that? scott fidel - deutsche bank...</td>\n",
              "      <td>0.273225</td>\n",
              "      <td>hi thanks, good morning. i want to talk a lit...</td>\n",
              "      <td>0.269723</td>\n",
              "      <td>sure. i think the question relates to nationa...</td>\n",
              "      <td>0.269054</td>\n",
              "      <td>yes, i think the market is postured for longe...</td>\n",
              "      <td>0.262632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3720</th>\n",
              "      <td>i mean, it’s kind of hard to ignore the numbe...</td>\n",
              "      <td>sure. this is jack larsen. just to answer you...</td>\n",
              "      <td>0.275454</td>\n",
              "      <td>just turning to medicaid in terms of the expe...</td>\n",
              "      <td>0.273857</td>\n",
              "      <td>i want to probe medicare advantage and let’s ...</td>\n",
              "      <td>0.271252</td>\n",
              "      <td>well, it is all about cost and there is you k...</td>\n",
              "      <td>0.270057</td>\n",
              "      <td>have the employers been proactivein asking yo...</td>\n",
              "      <td>0.265573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3721</th>\n",
              "      <td>sure. thanks steve. matt, i appreciate the qu...</td>\n",
              "      <td>a little bit of perspective on what’s happeni...</td>\n",
              "      <td>0.255249</td>\n",
              "      <td>and then, finally, when do you expect during ...</td>\n",
              "      <td>0.252560</td>\n",
              "      <td>and dcp, thank you for reminding, so dcp is d...</td>\n",
              "      <td>0.251789</td>\n",
              "      <td>yes. so if we look at, in particular -- so fi...</td>\n",
              "      <td>0.250678</td>\n",
              "      <td>sure. i will weigh into this and let dave dea...</td>\n",
              "      <td>0.247456</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3722 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4acdaa8-52a8-4eaf-a400-4b3cd644db98')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c4acdaa8-52a8-4eaf-a400-4b3cd644db98 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c4acdaa8-52a8-4eaf-a400-4b3cd644db98');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "We can see that the words \"medicare\", \"medicaid\" and \"kids\" have the most similarities to our documents. Considering the fact that our company is UnitedHealth Group Incorporated, these words make sense and have a close similarity to the top 5 documents. For the first 2 words, they also appear in the documents and are the topic of the answer/question.\n",
        "\n"
      ],
      "metadata": {
        "id": "CyHZqGTsqUhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict = {}\n",
        "for word in words:\n",
        "  word_dict[word] = ec2vec.wv.similar_by_word(word, topn = 5)"
      ],
      "metadata": {
        "id": "6VnUzWvU4H0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_cutoff = 0.80\n",
        "potentially_new_word = []\n",
        "for key in word_dict.keys():\n",
        "  if word_dict[key] == None:\n",
        "    continue\n",
        "  else:\n",
        "    for element in word_dict[key]:\n",
        "      new_word, cosine = element\n",
        "      if (cosine > cosine_cutoff) & (not(new_word in words)):\n",
        "        print(f'Adding new word: {new_word}')\n",
        "        potentially_new_word.append(new_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTqxONUj4Mtz",
        "outputId": "2fef1660-dab1-4d0b-f6cb-9a1f14c19f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding new word: esg\n",
            "Adding new word: renewables\n",
            "Adding new word: renewables\n",
            "Adding new word: renewables\n",
            "Adding new word: hydrogen\n",
            "Adding new word: urea\n",
            "Adding new word: fertilizer\n",
            "Adding new word: ammonia\n",
            "Adding new word: focused\n",
            "Adding new word: ensuring\n",
            "Adding new word: cultural\n",
            "Adding new word: airplanes\n",
            "Adding new word: submit\n",
            "Adding new word: determine\n",
            "Adding new word: consideration\n",
            "Adding new word: determine\n",
            "Adding new word: analysis\n",
            "Adding new word: allow\n",
            "Adding new word: determine\n",
            "Adding new word: consider\n",
            "Adding new word: decisions\n",
            "Adding new word: determine\n",
            "Adding new word: determine\n",
            "Adding new word: learn\n",
            "Adding new word: leaders\n",
            "Adding new word: team\n",
            "Adding new word: leader\n",
            "Adding new word: organization\n",
            "Adding new word: ceo\n",
            "Adding new word: dividend\n",
            "Adding new word: pensions\n",
            "Adding new word: loyalty\n",
            "Adding new word: presidential\n",
            "Adding new word: political\n",
            "Adding new word: political\n",
            "Adding new word: presidential\n",
            "Adding new word: proposed\n",
            "Adding new word: informed\n",
            "Adding new word: com\n",
            "Adding new word: attracted\n",
            "Adding new word: attracted\n",
            "Adding new word: leaders\n",
            "Adding new word: similarly\n",
            "Adding new word: meaning\n",
            "Adding new word: dynamic\n",
            "Adding new word: regardless\n",
            "Adding new word: broadly\n",
            "Adding new word: recently\n",
            "Adding new word: yesterday\n",
            "Adding new word: communication\n",
            "Adding new word: message\n",
            "Adding new word: engage\n",
            "Adding new word: stated\n",
            "Adding new word: reiterate\n",
            "Adding new word: previously\n",
            "Adding new word: confident\n",
            "Adding new word: outlined\n",
            "Adding new word: communication\n",
            "Adding new word: message\n",
            "Adding new word: pretty\n",
            "Adding new word: relatively\n",
            "Adding new word: although\n",
            "Adding new word: though\n",
            "Adding new word: somewhat\n",
            "Adding new word: slides\n",
            "Adding new word: presenting\n",
            "Adding new word: sustain\n",
            "Adding new word: sustained\n",
            "Adding new word: driving\n",
            "Adding new word: consistently\n",
            "Adding new word: specifics\n",
            "Adding new word: exact\n",
            "Adding new word: included\n",
            "Adding new word: include\n",
            "Adding new word: actual\n",
            "Adding new word: previously\n",
            "Adding new word: accounting\n",
            "Adding new word: release\n",
            "Adding new word: oxley\n",
            "Adding new word: engaging\n",
            "Adding new word: engage\n",
            "Adding new word: experiences\n",
            "Adding new word: engaged\n",
            "Adding new word: experience\n",
            "Adding new word: reception\n",
            "Adding new word: conference\n",
            "Adding new word: call\n",
            "Adding new word: concludes\n",
            "Adding new word: joining\n",
            "Adding new word: replay\n",
            "Adding new word: contact\n",
            "Adding new word: kids\n",
            "Adding new word: care\n",
            "Adding new word: healthcare\n",
            "Adding new word: medical\n",
            "Adding new word: remains\n",
            "Adding new word: continues\n",
            "Adding new word: robust\n",
            "Adding new word: continued\n",
            "Adding new word: strongly\n",
            "Adding new word: covid\n",
            "Adding new word: pre\n",
            "Adding new word: surge\n",
            "Adding new word: pent\n",
            "Adding new word: resilient\n",
            "Adding new word: drugs\n",
            "Adding new word: therapies\n",
            "Adding new word: therapy\n",
            "Adding new word: clinical\n",
            "Adding new word: efficacy\n",
            "Adding new word: payer\n",
            "Adding new word: hcv\n",
            "Adding new word: submission\n",
            "Adding new word: gdpr\n",
            "Adding new word: naturally\n",
            "Adding new word: helps\n",
            "Adding new word: meaning\n",
            "Adding new word: diverse\n",
            "Adding new word: diversification\n",
            "Adding new word: diversified\n",
            "Adding new word: breadth\n",
            "Adding new word: broad\n",
            "Adding new word: stake\n",
            "Adding new word: men\n",
            "Adding new word: apparel\n",
            "Adding new word: who\n",
            "Adding new word: their\n",
            "Adding new word: them\n",
            "Adding new word: they\n",
            "Adding new word: folks\n",
            "Adding new word: delivering\n",
            "Adding new word: importantly\n",
            "Adding new word: truly\n",
            "Adding new word: deliver\n",
            "Adding new word: demonstrating\n",
            "Adding new word: learn\n",
            "Adding new word: learnings\n",
            "Adding new word: learned\n",
            "Adding new word: scholarship\n",
            "Adding new word: trained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words += list(set(potentially_new_word))"
      ],
      "metadata": {
        "id": "eREQi4924Qp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normed_wv = ec2vec.wv.get_normed_vectors()\n",
        "word_indexes = ec2vec.wv.key_to_index\n",
        "\n",
        "words_indexes = [word_indexes[word] for word in words if word in vocab]\n",
        "norm_wordvecs = normed_wv[words_indexes, :]\n",
        "\n",
        "words_res = np.inner(norm_wordvecs, norm_docvecs)\n",
        "top_worddocs = np.flip(np.argsort(words_res, axis=1), axis=1)\n",
        "top_wordscores = np.flip(np.sort(words_res, axis=1), axis=1)\n",
        "\n",
        "words_with_highest_similiarity = np.flip(np.argsort(top_wordscores[:, 0]))"
      ],
      "metadata": {
        "id": "0_3aSoA32qCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word_id in words_with_highest_similiarity[:3]:\n",
        "  print('Word:\\n')\n",
        "  print(words[word_id])\n",
        "  print('\\n')\n",
        "\n",
        "  print('Five most similar documents:\\n')\n",
        "  for id_sim, sim_score in zip(top_worddocs[word_id, :5], top_wordscores[word_id, :5]):\n",
        "    print(f'Cosine similarity: {sim_score:.4f}\\n')\n",
        "    print(raw_docs[id_sim])\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZezbDPj26WG",
        "outputId": "91ee0d04-0fd7-45c7-c5c7-c7d1f8ebbebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:\n",
            "\n",
            "medicare\n",
            "\n",
            "\n",
            "Five most similar documents:\n",
            "\n",
            "Cosine similarity: 0.6923\n",
            "\n",
            " also on medicare advantage, can you comment on that?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.5710\n",
            "\n",
            " thanks. good morning. just wanted to clarify, on the higher acuities with the medicaid and medicare and the exchanges, i just want to make sure that the exchange mlr is within your expectations as well too.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.5670\n",
            "\n",
            " and that’s something new. and medicare had a very strong quarter in the fourth quarter and it came in line with expectations, it’s not a little bit ahead.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.5481\n",
            "\n",
            " and then, steve, you want to talk about medicaid?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.5341\n",
            "\n",
            " the last one was physician direct contracting fee-for-service medicare.\r\n",
            "\n",
            "\n",
            "Word:\n",
            "\n",
            "medicaid\n",
            "\n",
            "\n",
            "Five most similar documents:\n",
            "\n",
            "Cosine similarity: 0.6493\n",
            "\n",
            " and then, steve, you want to talk about medicaid?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.6285\n",
            "\n",
            " thanks. good morning. just wanted to clarify, on the higher acuities with the medicaid and medicare and the exchanges, i just want to make sure that the exchange mlr is within your expectations as well too.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.6022\n",
            "\n",
            " yes. hi, good morning. question on the medicaid side of the enrollment impact from higher unemployment is coming in lower than you expect. when do you expect the impact to peak? how do you think about the balance of going to medicaid versus exchanges? and then on the medicaid side, has the pandemic change how states think about transitioning to higher acuity populations to managed care fee-for-service and what type of visibility do you have for medicaid rates for next year at this point of time?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.5724\n",
            "\n",
            " i think it's a great question. i would include our medicaid capabilities in that as well. but tom, do you want to start?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.5703\n",
            "\n",
            " also on medicare advantage, can you comment on that?\r\n",
            "\n",
            "\n",
            "Word:\n",
            "\n",
            "kids\n",
            "\n",
            "\n",
            "Five most similar documents:\n",
            "\n",
            "Cosine similarity: 0.6103\n",
            "\n",
            " even though it is more kids and i assume more this is moms taking their kids to the doctor.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.3474\n",
            "\n",
            " well, it was a regional variation, so it wasn’t consistent across the board and what we saw is that, in those areas where the was a lot of news reports and tv going on, it didn’t matter whether you are a kid, an adult, a mom, that there were people going to the doctor and looking to see whether or not they had h1n1, even as they might not have had clear symptoms of the flu, just more to be cautious and obviously, there’s costs associated with that.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2810\n",
            "\n",
            " hey, good morning. how are you analyzing increased medical costs related to family coverage for children up to age 26 towards the end of the year and how much of your higher cost expectation in the second half of the year is related to that?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2562\n",
            "\n",
            " are you talking about specialty medications?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2469\n",
            "\n",
            " even though if you look at the cdc data, it was much greater in june than it has ever been for the flu before?\r\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "\n",
        "The documents closest to the words \"expect\" or \"expectation\" either have the words in the document or words similar to the such as \"forcast\". The topic of almost all of these documents is also financial aspects of company, growth, or costs. "
      ],
      "metadata": {
        "id": "nrCQ2AZHrj8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"expect\" in words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Wtmlau42EV",
        "outputId": "8b1d976f-a157-4bf0-b6da-1a9aeccf0bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words.append(\"expect\")\n",
        "words.append(\"expectation\")"
      ],
      "metadata": {
        "id": "jajryteHIzhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words.index(\"expect\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8mvC5iwJC4t",
        "outputId": "973d0561-b45b-4db2-836e-8678e8fb8f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "530"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_id = 530\n",
        "\n",
        "print('Word:\\n')\n",
        "print(words[word_id])\n",
        "print('\\n')\n",
        "\n",
        "print('Five most similar documents:\\n')\n",
        "for id_sim, sim_score in zip(top_worddocs[word_id, :5], top_wordscores[word_id, :5]):\n",
        "  print(f'Cosine similarity: {sim_score:.4f}\\n')\n",
        "  print(raw_docs[id_sim])\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FbI9g_z4lm5",
        "outputId": "e00010ff-4fd6-44dd-dbd9-71a3913cfd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:\n",
            "\n",
            "expect\n",
            "\n",
            "\n",
            "Five most similar documents:\n",
            "\n",
            "Cosine similarity: 0.2705\n",
            "\n",
            " commercial mlr, our expectation is similar to what we're seeing on a trend side. we would expect our loss ratio to be in that 82%, plus or minus 50 basis points but trending towards the lower end of that range.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2671\n",
            "\n",
            " we have anticipated that we would have somewhere between $3 and $3 ½ billion worth of dividends in 2008 and we expect to have about $2 billion for the remainder of the year.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2337\n",
            "\n",
            " no, we have been building the platform. so we have been incurring those costs.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2319\n",
            "\n",
            " it's negligible. when you look at it 2 months on the quarter, not even [ph]. no, it would not.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2283\n",
            "\n",
            " we did take down the commercial mlr from -- we were at 83.3% plus or minus 50 basis points on investor day and we took 83.5% plus or minus 50 basis points.\r\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words.index(\"expectation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W10MHmSRJZjN",
        "outputId": "c4b74bf5-2849-432c-a609-dcf4994153a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "531"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_id = 531\n",
        "\n",
        "print('Word:\\n')\n",
        "print(words[word_id])\n",
        "print('\\n')\n",
        "\n",
        "print('Five most similar documents:\\n')\n",
        "for id_sim, sim_score in zip(top_worddocs[word_id, :5], top_wordscores[word_id, :5]):\n",
        "  print(f'Cosine similarity: {sim_score:.4f}\\n')\n",
        "  print(raw_docs[id_sim])\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LdKCzpeJVmo",
        "outputId": "66dae646-7add-45f2-ed8a-490e74e20371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:\n",
            "\n",
            "expectation\n",
            "\n",
            "\n",
            "Five most similar documents:\n",
            "\n",
            "Cosine similarity: 0.2913\n",
            "\n",
            " it’s about a billion-dollar reduction from that original forecast.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2821\n",
            "\n",
            " commercial mlr, our expectation is similar to what we're seeing on a trend side. we would expect our loss ratio to be in that 82%, plus or minus 50 basis points but trending towards the lower end of that range.\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2526\n",
            "\n",
            " as we're approaching the end of the comment period for minimum loss ratio regs, do you anticipate any changes to those regs being passable? and can you tell us here what you're baking in your in 2011 guidance for the negative impact of the minimum mlr regs?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2427\n",
            "\n",
            " you’re not talking about cash flow; you’re talking about projected share repurchases?\r\n",
            "\n",
            "\n",
            "Cosine similarity: 0.2388\n",
            "\n",
            " we have anticipated that we would have somewhere between $3 and $3 ½ billion worth of dividends in 2008 and we expect to have about $2 billion for the remainder of the year.\r\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}